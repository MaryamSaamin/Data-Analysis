{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411167a2",
   "metadata": {},
   "source": [
    "## Read the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcaabb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Context  \\\n",
      "0  I'm going through some things with my feelings...   \n",
      "1  I'm going through some things with my feelings...   \n",
      "2  I'm going through some things with my feelings...   \n",
      "3  I'm going through some things with my feelings...   \n",
      "4  I'm going through some things with my feelings...   \n",
      "\n",
      "                                            Response  \n",
      "0  If everyone thinks you're worthless, then mayb...  \n",
      "1  Hello, and thank you for your question and see...  \n",
      "2  First thing I'd suggest is getting the sleep y...  \n",
      "3  Therapy is essential for those that are feelin...  \n",
      "4  I first want to let you know that you are not ...  \n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the zip file\n",
    "zip_file_path = r\"C:\\Users\\User\\Desktop\\Arcada\\Introduction to MLDP\\archive.zip\"\n",
    "\n",
    "# Directory to extract the files\n",
    "extract_dir = r\"C:\\Users\\User\\Desktop\\Arcada\\Introduction to MLDP\"\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Assuming train.csv is in the root directory of the zip file, read it\n",
    "csv_file_path = os.path.join(extract_dir, \"train.csv\")\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845d1c8",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3fdac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 1: Read the CSV file into a Pandas DataFrame\n",
    "#df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Step 2: Preprocess the text data\n",
    "df['Context'] = df['Context'].str.lower()\n",
    "df['Response'] = df['Response'].str.lower()\n",
    "df['Context'] = df['Context'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "df['Response'] = df['Response'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "\n",
    "# Replace missing values with a placeholder value\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Context'] + ' ' + df['Response'])\n",
    "df['Context'] = tokenizer.texts_to_sequences(df['Context'])\n",
    "df['Response'] = tokenizer.texts_to_sequences(df['Response'])\n",
    "\n",
    "# Step 4: Pad the sequences\n",
    "max_seq_length = 50  # Choose an appropriate value\n",
    "df['Context'] = pad_sequences(df['Context'], maxlen=max_seq_length, padding='post')\n",
    "df['Response'] = pad_sequences(df['Response'], maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Step 5: Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['Context'], df['Response'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, X_train, X_val, y_train, and y_val contain the processed data ready for training the RNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292a556",
   "metadata": {},
   "source": [
    "## Simple RNN and the Complex RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7946bed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "88/88 [==============================] - 6s 32ms/step - loss: 8.0323 - accuracy: 0.0424 - val_loss: 6.7393 - val_accuracy: 0.0384\n",
      "Epoch 2/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 6.1573 - accuracy: 0.0427 - val_loss: 6.5983 - val_accuracy: 0.0384\n",
      "Epoch 3/10\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 5.9949 - accuracy: 0.0449 - val_loss: 6.5740 - val_accuracy: 0.0384\n",
      "Epoch 4/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.9386 - accuracy: 0.0452 - val_loss: 6.5759 - val_accuracy: 0.0484\n",
      "Epoch 5/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.9087 - accuracy: 0.0427 - val_loss: 6.5895 - val_accuracy: 0.0384\n",
      "Epoch 6/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.8957 - accuracy: 0.0449 - val_loss: 6.6027 - val_accuracy: 0.0384\n",
      "Epoch 7/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.8850 - accuracy: 0.0449 - val_loss: 6.6126 - val_accuracy: 0.0384\n",
      "Epoch 8/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.8812 - accuracy: 0.0449 - val_loss: 6.6313 - val_accuracy: 0.0398\n",
      "Epoch 9/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.8733 - accuracy: 0.0427 - val_loss: 6.6417 - val_accuracy: 0.0384\n",
      "Epoch 10/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.8651 - accuracy: 0.0449 - val_loss: 6.6527 - val_accuracy: 0.0384\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 6.6527 - accuracy: 0.0384\n",
      "Validation Loss: 6.652738094329834\n",
      "Validation Accuracy: 0.03840682655572891\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Define the Simple RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=64, activation='tanh', input_shape=(max_seq_length, 1)))\n",
    "model.add(Dense(units=len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data to 3D\n",
    "X_train_3d = np.expand_dims(X_train, axis=-1)\n",
    "X_val_3d = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "history = model.fit(X_train_3d, np.array(y_train), validation_data=(X_val_3d, np.array(y_val)), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val_3d, np.array(y_val))\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e341a76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "88/88 [==============================] - 8s 41ms/step - loss: 8.5293 - accuracy: 0.0370 - val_loss: 6.8193 - val_accuracy: 0.0384\n",
      "Epoch 2/10\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 6.2207 - accuracy: 0.0441 - val_loss: 6.7505 - val_accuracy: 0.0384\n",
      "Epoch 3/10\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.0259 - accuracy: 0.0413 - val_loss: 6.7135 - val_accuracy: 0.0384\n",
      "Epoch 4/10\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.9380 - accuracy: 0.0441 - val_loss: 6.6931 - val_accuracy: 0.0484\n",
      "Epoch 5/10\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.8996 - accuracy: 0.0441 - val_loss: 6.6792 - val_accuracy: 0.0484\n",
      "Epoch 6/10\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.8770 - accuracy: 0.0402 - val_loss: 6.6908 - val_accuracy: 0.0384\n",
      "Epoch 7/10\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.8562 - accuracy: 0.0431 - val_loss: 6.6879 - val_accuracy: 0.0384\n",
      "Epoch 8/10\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 5.8344 - accuracy: 0.0427 - val_loss: 6.6833 - val_accuracy: 0.0384\n",
      "Epoch 9/10\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.8083 - accuracy: 0.0459 - val_loss: 6.6877 - val_accuracy: 0.0384\n",
      "Epoch 10/10\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 5.7727 - accuracy: 0.0452 - val_loss: 6.6612 - val_accuracy: 0.0384\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 6.6612 - accuracy: 0.0384\n",
      "Validation Loss: 6.66124963760376\n",
      "Validation Accuracy: 0.03840682655572891\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 100)           1937900   \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 50, 128)           29312     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50, 128)           0         \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 64)                12352     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 19379)             1259635   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3239199 (12.36 MB)\n",
      "Trainable params: 3239199 (12.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
    "\n",
    "# Define the SimpleRNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_seq_length))\n",
    "model.add(SimpleRNN(units=128, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))  # Adding dropout regularization\n",
    "model.add(SimpleRNN(units=64, activation='tanh'))\n",
    "model.add(Dropout(0.2))  # Adding dropout regularization\n",
    "model.add(Dense(units=len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data to 3D\n",
    "X_train_3d = np.expand_dims(X_train, axis=-1)\n",
    "X_val_3d = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "history = model.fit(X_train_3d, np.array(y_train), validation_data=(X_val_3d, np.array(y_val)), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val_3d, np.array(y_val))\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66722246",
   "metadata": {},
   "source": [
    "## Simple LSTM and the Complex LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67291d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "88/88 [==============================] - 14s 51ms/step - loss: 9.5891 - accuracy: 0.0402 - val_loss: 7.9781 - val_accuracy: 0.0384\n",
      "Epoch 2/10\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 6.4890 - accuracy: 0.0449 - val_loss: 6.7666 - val_accuracy: 0.0384\n",
      "Epoch 3/10\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 5.9960 - accuracy: 0.0417 - val_loss: 6.6973 - val_accuracy: 0.0384\n",
      "Epoch 4/10\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 5.9120 - accuracy: 0.0449 - val_loss: 6.6830 - val_accuracy: 0.0384\n",
      "Epoch 5/10\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 5.8720 - accuracy: 0.0427 - val_loss: 6.6733 - val_accuracy: 0.0384\n",
      "Epoch 6/10\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 5.8458 - accuracy: 0.0449 - val_loss: 6.6511 - val_accuracy: 0.0384\n",
      "Epoch 7/10\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 5.8309 - accuracy: 0.0427 - val_loss: 6.6843 - val_accuracy: 0.0384\n",
      "Epoch 8/10\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 5.8230 - accuracy: 0.0449 - val_loss: 6.6888 - val_accuracy: 0.0384\n",
      "Epoch 9/10\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 5.8149 - accuracy: 0.0449 - val_loss: 6.6850 - val_accuracy: 0.0384\n",
      "Epoch 10/10\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 5.8087 - accuracy: 0.0449 - val_loss: 6.6863 - val_accuracy: 0.0384\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 6.6863 - accuracy: 0.0384\n",
      "Validation Loss: 6.6863017082214355\n",
      "Validation Accuracy: 0.03840682655572891\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_seq_length))\n",
    "model.add(LSTM(units=64, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(units=64, activation='tanh'))\n",
    "model.add(Dense(units=len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data to 3D\n",
    "X_train_3d = np.expand_dims(X_train, axis=-1)\n",
    "X_val_3d = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "history = model.fit(X_train_3d, np.array(y_train), validation_data=(X_val_3d, np.array(y_val)), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val_3d, np.array(y_val))\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28b4cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "88/88 [==============================] - 14s 56ms/step - loss: 9.3880 - accuracy: 0.0417 - val_loss: 7.2852 - val_accuracy: 0.0384\n",
      "Epoch 2/10\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 6.4069 - accuracy: 0.0438 - val_loss: 6.7632 - val_accuracy: 0.0384\n",
      "Epoch 3/10\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 6.0540 - accuracy: 0.0449 - val_loss: 6.7382 - val_accuracy: 0.0384\n",
      "Epoch 4/10\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 5.9608 - accuracy: 0.0434 - val_loss: 6.7369 - val_accuracy: 0.0384\n",
      "Epoch 5/10\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 5.9138 - accuracy: 0.0449 - val_loss: 6.7126 - val_accuracy: 0.0384\n",
      "Epoch 6/10\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 5.8827 - accuracy: 0.0420 - val_loss: 6.7082 - val_accuracy: 0.0384\n",
      "Epoch 7/10\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 5.8577 - accuracy: 0.0449 - val_loss: 6.7038 - val_accuracy: 0.0384\n",
      "Epoch 8/10\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 5.8498 - accuracy: 0.0413 - val_loss: 6.7100 - val_accuracy: 0.0384\n",
      "Epoch 9/10\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 5.8381 - accuracy: 0.0452 - val_loss: 6.7218 - val_accuracy: 0.0384\n",
      "Epoch 10/10\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 5.8352 - accuracy: 0.0449 - val_loss: 6.7343 - val_accuracy: 0.0384\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 6.7343 - accuracy: 0.0384\n",
      "Validation Loss: 6.734299182891846\n",
      "Validation Accuracy: 0.03840682655572891\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 100)           1937900   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50, 128)           117248    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50, 128)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 19379)             1259635   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3364191 (12.83 MB)\n",
      "Trainable params: 3364191 (12.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_seq_length))\n",
    "model.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))  # Adding dropout regularization\n",
    "model.add(LSTM(units=64, activation='tanh'))\n",
    "model.add(Dropout(0.2))  # Adding dropout regularization\n",
    "model.add(Dense(units=len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data to 3D\n",
    "X_train_3d = np.expand_dims(X_train, axis=-1)\n",
    "X_val_3d = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "history = model.fit(X_train_3d, np.array(y_train), validation_data=(X_val_3d, np.array(y_val)), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val_3d, np.array(y_val))\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7add8",
   "metadata": {},
   "source": [
    "## Simple GRU and the Complex GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8581bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "88/88 [==============================] - 7s 34ms/step - loss: 8.5089 - accuracy: 0.0413 - val_loss: 6.9237 - val_accuracy: 0.0384\n",
      "Epoch 2/10\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 6.1654 - accuracy: 0.0427 - val_loss: 6.5649 - val_accuracy: 0.0384\n",
      "Epoch 3/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.9071 - accuracy: 0.0434 - val_loss: 6.5385 - val_accuracy: 0.0384\n",
      "Epoch 4/10\n",
      "88/88 [==============================] - 2s 20ms/step - loss: 5.8239 - accuracy: 0.0424 - val_loss: 6.5343 - val_accuracy: 0.0384\n",
      "Epoch 5/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.7746 - accuracy: 0.0449 - val_loss: 6.5369 - val_accuracy: 0.0384\n",
      "Epoch 6/10\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 5.7408 - accuracy: 0.0409 - val_loss: 6.5398 - val_accuracy: 0.0384\n",
      "Epoch 7/10\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 5.7131 - accuracy: 0.0449 - val_loss: 6.5495 - val_accuracy: 0.0384\n",
      "Epoch 8/10\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 5.6942 - accuracy: 0.0399 - val_loss: 6.5485 - val_accuracy: 0.0384\n",
      "Epoch 9/10\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.6752 - accuracy: 0.0445 - val_loss: 6.5671 - val_accuracy: 0.0384\n",
      "Epoch 10/10\n",
      "88/88 [==============================] - 2s 18ms/step - loss: 5.6579 - accuracy: 0.0409 - val_loss: 6.5571 - val_accuracy: 0.0384\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 6.5571 - accuracy: 0.0384\n",
      "Validation Loss: 6.557051658630371\n",
      "Validation Accuracy: 0.03840682655572891\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential()\n",
    "model.add(GRU(units=64, activation='tanh', input_shape=(max_seq_length, 1)))\n",
    "model.add(Dense(units=len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data to 3D\n",
    "X_train_3d = np.expand_dims(X_train, axis=-1)\n",
    "X_val_3d = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "history = model.fit(X_train_3d, np.array(y_train), validation_data=(X_val_3d, np.array(y_val)), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val_3d, np.array(y_val))\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138e8448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "88/88 [==============================] - 16s 59ms/step - loss: 8.8428 - accuracy: 0.0388 - val_loss: 6.8270 - val_accuracy: 0.0384\n",
      "Epoch 2/10\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 6.2746 - accuracy: 0.0456 - val_loss: 6.7302 - val_accuracy: 0.0384\n",
      "Epoch 3/10\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 6.0199 - accuracy: 0.0441 - val_loss: 6.6672 - val_accuracy: 0.0384\n",
      "Epoch 4/10\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 5.9357 - accuracy: 0.0438 - val_loss: 6.6557 - val_accuracy: 0.0384\n",
      "Epoch 5/10\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 5.8898 - accuracy: 0.0449 - val_loss: 6.6436 - val_accuracy: 0.0384\n",
      "Epoch 6/10\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 5.8669 - accuracy: 0.0449 - val_loss: 6.6423 - val_accuracy: 0.0384\n",
      "Epoch 7/10\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 5.8523 - accuracy: 0.0449 - val_loss: 6.6514 - val_accuracy: 0.0384\n",
      "Epoch 8/10\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 5.8419 - accuracy: 0.0449 - val_loss: 6.6572 - val_accuracy: 0.0384\n",
      "Epoch 9/10\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 5.8245 - accuracy: 0.0445 - val_loss: 6.6717 - val_accuracy: 0.0384\n",
      "Epoch 10/10\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 5.8148 - accuracy: 0.0452 - val_loss: 6.7018 - val_accuracy: 0.0384\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 6.7018 - accuracy: 0.0384\n",
      "Validation Loss: 6.701846122741699\n",
      "Validation Accuracy: 0.03840682655572891\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 50, 100)           1937900   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 50, 128)           88320     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 50, 128)           0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 19379)             1259635   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3323103 (12.68 MB)\n",
      "Trainable params: 3323103 (12.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Embedding, Dropout\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_seq_length))\n",
    "model.add(GRU(units=128, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))  # Adding dropout regularization\n",
    "model.add(GRU(units=64, activation='tanh'))\n",
    "model.add(Dropout(0.2))  # Adding dropout regularization\n",
    "model.add(Dense(units=len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data to 3D\n",
    "X_train_3d = np.expand_dims(X_train, axis=-1)\n",
    "X_val_3d = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "history = model.fit(X_train_3d, np.array(y_train), validation_data=(X_val_3d, np.array(y_val)), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val_3d, np.array(y_val))\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e5b21",
   "metadata": {},
   "source": [
    "## Evaluate 3 models with simple network and same number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cac19db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 - 19s - loss: 7.8184 - accuracy: 7.1276e-04 - val_loss: 7.8736 - val_accuracy: 0.0000e+00 - 19s/epoch - 442ms/step\n",
      "Epoch 2/10\n",
      "44/44 - 16s - loss: 7.8028 - accuracy: 7.1276e-04 - val_loss: 7.7976 - val_accuracy: 0.0000e+00 - 16s/epoch - 364ms/step\n",
      "Epoch 3/10\n",
      "44/44 - 16s - loss: 7.8140 - accuracy: 0.0014 - val_loss: 7.8841 - val_accuracy: 0.0000e+00 - 16s/epoch - 364ms/step\n",
      "Epoch 4/10\n",
      "44/44 - 16s - loss: 7.8298 - accuracy: 3.5638e-04 - val_loss: 7.9706 - val_accuracy: 0.0000e+00 - 16s/epoch - 362ms/step\n",
      "Epoch 5/10\n",
      "44/44 - 16s - loss: 7.8360 - accuracy: 0.0000e+00 - val_loss: 7.8861 - val_accuracy: 0.0000e+00 - 16s/epoch - 365ms/step\n",
      "Epoch 6/10\n",
      "44/44 - 16s - loss: 7.8185 - accuracy: 0.0014 - val_loss: 8.0066 - val_accuracy: 0.0000e+00 - 16s/epoch - 364ms/step\n",
      "Epoch 7/10\n",
      "44/44 - 16s - loss: 7.8119 - accuracy: 0.0011 - val_loss: 8.1195 - val_accuracy: 0.0000e+00 - 16s/epoch - 359ms/step\n",
      "Epoch 8/10\n",
      "44/44 - 16s - loss: 7.7660 - accuracy: 0.0018 - val_loss: 7.9654 - val_accuracy: 0.0000e+00 - 16s/epoch - 368ms/step\n",
      "Epoch 9/10\n",
      "44/44 - 16s - loss: 7.7162 - accuracy: 0.0018 - val_loss: 8.1070 - val_accuracy: 0.0000e+00 - 16s/epoch - 368ms/step\n",
      "Epoch 10/10\n",
      "44/44 - 16s - loss: 7.6338 - accuracy: 0.0043 - val_loss: 8.0524 - val_accuracy: 0.0000e+00 - 16s/epoch - 360ms/step\n",
      "Epoch 1/10\n",
      "44/44 - 80s - loss: 7.8184 - accuracy: 0.0025 - val_loss: 7.8188 - val_accuracy: 0.0000e+00 - 80s/epoch - 2s/step\n",
      "Epoch 2/10\n",
      "44/44 - 74s - loss: 7.8013 - accuracy: 0.0046 - val_loss: 7.8406 - val_accuracy: 0.0014 - 74s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "44/44 - 73s - loss: 7.7729 - accuracy: 0.0046 - val_loss: 7.8956 - val_accuracy: 0.0000e+00 - 73s/epoch - 2s/step\n",
      "Epoch 4/10\n",
      "44/44 - 75s - loss: 7.6819 - accuracy: 0.0029 - val_loss: 8.1086 - val_accuracy: 0.0000e+00 - 75s/epoch - 2s/step\n",
      "Epoch 5/10\n",
      "44/44 - 75s - loss: 7.3969 - accuracy: 0.0050 - val_loss: 8.4534 - val_accuracy: 0.0000e+00 - 75s/epoch - 2s/step\n",
      "Epoch 6/10\n",
      "44/44 - 74s - loss: 6.9268 - accuracy: 0.0064 - val_loss: 8.5516 - val_accuracy: 0.0000e+00 - 74s/epoch - 2s/step\n",
      "Epoch 7/10\n",
      "44/44 - 75s - loss: 6.3718 - accuracy: 0.0285 - val_loss: 8.5247 - val_accuracy: 0.0000e+00 - 75s/epoch - 2s/step\n",
      "Epoch 8/10\n",
      "44/44 - 74s - loss: 5.8323 - accuracy: 0.0823 - val_loss: 8.6216 - val_accuracy: 0.0000e+00 - 74s/epoch - 2s/step\n",
      "Epoch 9/10\n",
      "44/44 - 75s - loss: 5.3291 - accuracy: 0.1572 - val_loss: 8.5758 - val_accuracy: 0.0000e+00 - 75s/epoch - 2s/step\n",
      "Epoch 10/10\n",
      "44/44 - 75s - loss: 4.8704 - accuracy: 0.2028 - val_loss: 8.4890 - val_accuracy: 0.0014 - 75s/epoch - 2s/step\n",
      "Epoch 1/10\n",
      "44/44 - 79s - loss: 7.8183 - accuracy: 0.0057 - val_loss: 7.8199 - val_accuracy: 0.0043 - 79s/epoch - 2s/step\n",
      "Epoch 2/10\n",
      "44/44 - 72s - loss: 7.7902 - accuracy: 0.0107 - val_loss: 7.8540 - val_accuracy: 0.0000e+00 - 72s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "44/44 - 75s - loss: 7.6023 - accuracy: 0.0096 - val_loss: 8.0271 - val_accuracy: 0.0000e+00 - 75s/epoch - 2s/step\n",
      "Epoch 4/10\n",
      "44/44 - 75s - loss: 7.1510 - accuracy: 0.0050 - val_loss: 8.3322 - val_accuracy: 0.0000e+00 - 75s/epoch - 2s/step\n",
      "Epoch 5/10\n",
      "44/44 - 75s - loss: 6.6178 - accuracy: 0.0125 - val_loss: 8.3739 - val_accuracy: 0.0000e+00 - 75s/epoch - 2s/step\n",
      "Epoch 6/10\n",
      "44/44 - 75s - loss: 6.0700 - accuracy: 0.0481 - val_loss: 8.4255 - val_accuracy: 0.0000e+00 - 75s/epoch - 2s/step\n",
      "Epoch 7/10\n",
      "44/44 - 74s - loss: 5.5375 - accuracy: 0.1304 - val_loss: 8.2946 - val_accuracy: 0.0028 - 74s/epoch - 2s/step\n",
      "Epoch 8/10\n",
      "44/44 - 70s - loss: 5.0476 - accuracy: 0.1796 - val_loss: 8.2698 - val_accuracy: 0.0071 - 70s/epoch - 2s/step\n",
      "Epoch 9/10\n",
      "44/44 - 66s - loss: 4.6058 - accuracy: 0.2195 - val_loss: 8.1123 - val_accuracy: 0.0171 - 66s/epoch - 1s/step\n",
      "Epoch 10/10\n",
      "44/44 - 62s - loss: 4.2055 - accuracy: 0.2366 - val_loss: 8.0888 - val_accuracy: 0.0342 - 62s/epoch - 1s/step\n",
      "22/22 [==============================] - 2s 65ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 5s 186ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 5s 182ms/step\n",
      "Performance Metrics:\n",
      "Basic RNN - Accuracy: 0.0000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "LSTM - Accuracy: 0.0014, Precision: 0.0007, Recall: 0.0014, F1 Score: 0.0009\n",
      "GRU - Accuracy: 0.0342, Precision: 0.0269, Recall: 0.0342, F1 Score: 0.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Drop rows with NaN values in the \"Response\" column\n",
    "df = df.dropna(subset=['Response'])\n",
    "\n",
    "# Convert the \"Response\" column to string type\n",
    "df['Response'] = df['Response'].astype(str)\n",
    "\n",
    "# Preprocess the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Context'])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(df['Context'])\n",
    "padded_input_sequences = pad_sequences(input_sequences)\n",
    "\n",
    "# Label encode the \"Response\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df['Response'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_input_sequences, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Determine the number of unique classes in the target variable\n",
    "num_classes = len(np.unique(y_encoded))\n",
    "\n",
    "# Build and train a basic RNN model\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(total_words, 100, input_length=X_train.shape[1]))\n",
    "model_rnn.add(SimpleRNN(100))\n",
    "model_rnn.add(Dense(num_classes, activation='softmax'))\n",
    "model_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Build and train an LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(total_words, 100, input_length=X_train.shape[1]))\n",
    "model_lstm.add(LSTM(100))\n",
    "model_lstm.add(Dense(num_classes, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Build and train a GRU model\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Embedding(total_words, 100, input_length=X_train.shape[1]))\n",
    "model_gru.add(GRU(100))\n",
    "model_gru.add(Dense(num_classes, activation='softmax'))\n",
    "model_gru.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_gru.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy_rnn, precision_rnn, recall_rnn, f1_rnn = evaluate_model(model_rnn, X_test, y_test)\n",
    "accuracy_lstm, precision_lstm, recall_lstm, f1_lstm = evaluate_model(model_lstm, X_test, y_test)\n",
    "accuracy_gru, precision_gru, recall_gru, f1_gru = evaluate_model(model_gru, X_test, y_test)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(\"Basic RNN - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_rnn, precision_rnn, recall_rnn, f1_rnn))\n",
    "print(\"LSTM - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_lstm, precision_lstm, recall_lstm, f1_lstm))\n",
    "print(\"GRU - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_gru, precision_gru, recall_gru, f1_gru))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ecb2f",
   "metadata": {},
   "source": [
    "## Evaluate 3 models with complex network and different number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "098f0cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 - 27s - loss: 7.8447 - accuracy: 3.5600e-04 - val_loss: 7.8463 - val_accuracy: 0.0000e+00 - 27s/epoch - 611ms/step\n",
      "Epoch 2/10\n",
      "44/44 - 24s - loss: 7.8057 - accuracy: 3.5600e-04 - val_loss: 8.0537 - val_accuracy: 0.0000e+00 - 24s/epoch - 540ms/step\n",
      "Epoch 3/10\n",
      "44/44 - 23s - loss: 7.7612 - accuracy: 7.1200e-04 - val_loss: 8.1766 - val_accuracy: 0.0000e+00 - 23s/epoch - 523ms/step\n",
      "Epoch 4/10\n",
      "44/44 - 23s - loss: 7.7464 - accuracy: 3.5600e-04 - val_loss: 8.3952 - val_accuracy: 0.0000e+00 - 23s/epoch - 520ms/step\n",
      "Epoch 5/10\n",
      "44/44 - 23s - loss: 7.7272 - accuracy: 7.1200e-04 - val_loss: 8.4402 - val_accuracy: 0.0000e+00 - 23s/epoch - 513ms/step\n",
      "Epoch 6/10\n",
      "44/44 - 24s - loss: 7.7069 - accuracy: 0.0011 - val_loss: 8.6066 - val_accuracy: 0.0000e+00 - 24s/epoch - 534ms/step\n",
      "Epoch 7/10\n",
      "44/44 - 24s - loss: 7.6942 - accuracy: 0.0014 - val_loss: 8.6792 - val_accuracy: 0.0000e+00 - 24s/epoch - 536ms/step\n",
      "Epoch 8/10\n",
      "44/44 - 19s - loss: 7.6790 - accuracy: 0.0014 - val_loss: 9.0447 - val_accuracy: 0.0000e+00 - 19s/epoch - 443ms/step\n",
      "Epoch 9/10\n",
      "44/44 - 21s - loss: 7.6707 - accuracy: 0.0014 - val_loss: 9.1195 - val_accuracy: 0.0000e+00 - 21s/epoch - 472ms/step\n",
      "Epoch 10/10\n",
      "44/44 - 19s - loss: 7.6634 - accuracy: 0.0014 - val_loss: 9.2940 - val_accuracy: 0.0000e+00 - 19s/epoch - 439ms/step\n",
      "Epoch 1/10\n",
      "44/44 - 152s - loss: 7.8210 - accuracy: 3.5600e-04 - val_loss: 7.8275 - val_accuracy: 0.0000e+00 - 152s/epoch - 3s/step\n",
      "Epoch 2/10\n",
      "44/44 - 151s - loss: 7.8026 - accuracy: 7.1200e-04 - val_loss: 7.9294 - val_accuracy: 0.0000e+00 - 151s/epoch - 3s/step\n",
      "Epoch 3/10\n",
      "44/44 - 158s - loss: 7.6419 - accuracy: 0.0000e+00 - val_loss: 8.2447 - val_accuracy: 0.0000e+00 - 158s/epoch - 4s/step\n",
      "Epoch 4/10\n",
      "44/44 - 152s - loss: 7.2908 - accuracy: 0.0011 - val_loss: 8.7960 - val_accuracy: 0.0000e+00 - 152s/epoch - 3s/step\n",
      "Epoch 5/10\n",
      "44/44 - 160s - loss: 6.9275 - accuracy: 0.0018 - val_loss: 9.1167 - val_accuracy: 0.0000e+00 - 160s/epoch - 4s/step\n",
      "Epoch 6/10\n",
      "44/44 - 167s - loss: 6.6619 - accuracy: 0.0014 - val_loss: 9.5074 - val_accuracy: 0.0000e+00 - 167s/epoch - 4s/step\n",
      "Epoch 7/10\n",
      "44/44 - 174s - loss: 6.4258 - accuracy: 0.0028 - val_loss: 9.8036 - val_accuracy: 0.0000e+00 - 174s/epoch - 4s/step\n",
      "Epoch 8/10\n",
      "44/44 - 167s - loss: 6.2110 - accuracy: 0.0028 - val_loss: 10.0286 - val_accuracy: 0.0000e+00 - 167s/epoch - 4s/step\n",
      "Epoch 9/10\n",
      "44/44 - 168s - loss: 6.0381 - accuracy: 0.0039 - val_loss: 10.2007 - val_accuracy: 0.0000e+00 - 168s/epoch - 4s/step\n",
      "Epoch 10/10\n",
      "44/44 - 169s - loss: 5.8829 - accuracy: 0.0057 - val_loss: 10.3665 - val_accuracy: 0.0000e+00 - 169s/epoch - 4s/step\n",
      "Epoch 1/10\n",
      "44/44 - 253s - loss: 7.8213 - accuracy: 0.0000e+00 - val_loss: 7.8281 - val_accuracy: 0.0014 - 253s/epoch - 6s/step\n",
      "Epoch 2/10\n",
      "44/44 - 259s - loss: 7.7971 - accuracy: 0.0018 - val_loss: 7.8826 - val_accuracy: 0.0000e+00 - 259s/epoch - 6s/step\n",
      "Epoch 3/10\n",
      "44/44 - 268s - loss: 7.4977 - accuracy: 0.0025 - val_loss: 8.2372 - val_accuracy: 0.0000e+00 - 268s/epoch - 6s/step\n",
      "Epoch 4/10\n",
      "44/44 - 288s - loss: 6.6561 - accuracy: 0.0110 - val_loss: 9.2244 - val_accuracy: 0.0000e+00 - 288s/epoch - 7s/step\n",
      "Epoch 5/10\n",
      "44/44 - 292s - loss: 5.4335 - accuracy: 0.0655 - val_loss: 9.9515 - val_accuracy: 0.0043 - 292s/epoch - 7s/step\n",
      "Epoch 6/10\n",
      "44/44 - 278s - loss: 4.2005 - accuracy: 0.1396 - val_loss: 10.6351 - val_accuracy: 0.0242 - 278s/epoch - 6s/step\n",
      "Epoch 7/10\n",
      "44/44 - 280s - loss: 3.2972 - accuracy: 0.1976 - val_loss: 11.0048 - val_accuracy: 0.1038 - 280s/epoch - 6s/step\n",
      "Epoch 8/10\n",
      "44/44 - 268s - loss: 2.7642 - accuracy: 0.2375 - val_loss: 11.4022 - val_accuracy: 0.1650 - 268s/epoch - 6s/step\n",
      "Epoch 9/10\n",
      "44/44 - 248s - loss: 2.4357 - accuracy: 0.2844 - val_loss: 11.0458 - val_accuracy: 0.1849 - 248s/epoch - 6s/step\n",
      "Epoch 10/10\n",
      "44/44 - 143s - loss: 2.2576 - accuracy: 0.2884 - val_loss: 11.1579 - val_accuracy: 0.1878 - 143s/epoch - 3s/step\n",
      "22/22 [==============================] - 2s 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 5s 189ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 9s 369ms/step\n",
      "Performance Metrics:\n",
      "Basic RNN - Accuracy: 0.0000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "LSTM - Accuracy: 0.0000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "GRU - Accuracy: 0.1878, Precision: 0.1636, Recall: 0.1878, F1 Score: 0.1707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Step 2: Preprocess the text data\n",
    "df['Context'] = df['Context'].str.lower()\n",
    "df['Response'] = df['Response'].str.lower()\n",
    "df['Context'] = df['Context'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "df['Response'] = df['Response'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "\n",
    "# Replace missing values with a placeholder value\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Drop rows with NaN values in the \"Response\" column\n",
    "df = df.dropna(subset=['Response'])\n",
    "\n",
    "# Convert the \"Response\" column to string type\n",
    "df['Response'] = df['Response'].astype(str)\n",
    "\n",
    "# Preprocess the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Context'])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(df['Context'])\n",
    "padded_input_sequences = pad_sequences(input_sequences)\n",
    "\n",
    "# Label encode the \"Response\" column\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df['Response'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_input_sequences, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Determine the number of unique classes in the target variable\n",
    "num_classes = len(np.unique(y_encoded))\n",
    "\n",
    "# Build and train a basic RNN model\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(total_words, 100, input_length=X_train.shape[1]))\n",
    "model_rnn.add(SimpleRNN(128, return_sequences=True))  # Adding return_sequences=True for stacking RNN layers\n",
    "model_rnn.add(SimpleRNN(128))  # Adding another RNN layer\n",
    "model_rnn.add(Dense(64, activation='relu'))  # Adding a dense layer for more complexity\n",
    "model_rnn.add(Dense(num_classes, activation='softmax'))\n",
    "model_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Build and train an LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(total_words, 100, input_length=X_train.shape[1]))\n",
    "model_lstm.add(LSTM(128, return_sequences=True))  # Adding return_sequences=True for stacking LSTM layers\n",
    "model_lstm.add(LSTM(128))  # Adding another LSTM layer\n",
    "model_lstm.add(Dense(64, activation='relu'))  # Adding a dense layer for more complexity\n",
    "model_lstm.add(Dense(num_classes, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Build and train a GRU model\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Embedding(total_words, 100, input_length=X_train.shape[1]))\n",
    "model_gru.add(GRU(256, return_sequences=True))  # Adding return_sequences=True for stacking GRU layers\n",
    "model_gru.add(GRU(256))  # Adding another GRU layer\n",
    "model_gru.add(Dense(128, activation='relu'))  # Adding a dense layer for more complexity\n",
    "model_gru.add(Dense(num_classes, activation='softmax'))\n",
    "model_gru.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_gru.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy_rnn, precision_rnn, recall_rnn, f1_rnn = evaluate_model(model_rnn, X_test, y_test)\n",
    "accuracy_lstm, precision_lstm, recall_lstm, f1_lstm = evaluate_model(model_lstm, X_test, y_test)\n",
    "accuracy_gru, precision_gru, recall_gru, f1_gru = evaluate_model(model_gru, X_test, y_test)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(\"Basic RNN - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_rnn, precision_rnn, recall_rnn, f1_rnn))\n",
    "print(\"LSTM - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_lstm, precision_lstm, recall_lstm, f1_lstm))\n",
    "print(\"GRU - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_gru, precision_gru, recall_gru, f1_gru))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b62136a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 529, 100)          378800    \n",
      "                                                                 \n",
      " simple_rnn_4 (SimpleRNN)    (None, 529, 128)          29312     \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 2480)              161200    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 610464 (2.33 MB)\n",
      "Trainable params: 610464 (2.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 529, 100)          378800    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 529, 128)          117248    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 2480)              161200    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 797088 (3.04 MB)\n",
      "Trainable params: 797088 (3.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 529, 100)          378800    \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 529, 256)          274944    \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 256)               394752    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 2480)              319920    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1401312 (5.35 MB)\n",
      "Trainable params: 1401312 (5.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_rnn.summary())\n",
    "print(model_lstm.summary())\n",
    "print(model_gru.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b449c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
